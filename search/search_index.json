{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":""},{"location":"#aws-lambda-handler-cookbook-a-serverless-service-blueprint","title":"AWS Lambda Handler Cookbook - A Serverless Service Blueprint","text":""},{"location":"#aws-recommendation","title":"AWS Recommendation","text":"<p>This repository was recommended in an AWS blog post Best practices for accelerating development with serverless blueprints</p> <p></p>"},{"location":"#concepts","title":"Concepts","text":"<p>I spoke at AWS re:invent 2023 with Heitor Lessa, former Chief Architect of Powertools for AWS Lambda about the concepts I implemented in this project.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Starting a Serverless service can be overwhelming. You need to figure out many questions and challenges that have nothing to do with your business domain:</p> <ul> <li>How to deploy to the cloud? What IAC framework do you choose?</li> <li>How to write a SaaS-oriented CI/CD pipeline? What does it need to contain?</li> <li>How do you handle observability, logging, tracing, metrics?</li> <li>How do you handle testing?</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>This project aims to reduce cognitive load and answer these questions for you by providing a skeleton Python Serverless service blueprint that implements best practices for AWS Lambda, Serverless CI/CD, and AWS CDK in one blueprint project.</p>"},{"location":"#serverless-service-the-order-service","title":"Serverless Service - The Order service","text":"<ul> <li> <p>This project provides a working orders service where customers can create orders of items.</p> </li> <li> <p>The project deploys an API GW with an AWS Lambda integration under the path POST /api/orders/ and stores orders data in a DynamoDB table.</p> </li> </ul>"},{"location":"#monitoring-design","title":"Monitoring Design","text":""},{"location":"#features","title":"Features","text":"<ul> <li>Python Serverless service with a recommended file structure.</li> <li>CDK infrastructure with infrastructure tests and security tests.</li> <li>CI/CD pipelines based on Github actions that deploys to AWS with python linters, complexity checks and style formatters.</li> <li>CI/CD pipeline deploys to dev/staging and production environment with different gates between each environment</li> <li>Makefile for simple developer experience.</li> <li>The AWS Lambda handler embodies Serverless best practices and has all the bells and whistles for a proper production ready handler.</li> <li>AWS Lambda handler uses AWS Lambda Powertools.</li> <li>AWS Lambda handler 3 layer architecture: handler layer, logic layer and data access layer</li> <li>Features flags and configuration based on AWS AppConfig</li> <li>CloudWatch dashboards - High level and low level including CloudWatch alarms</li> <li>Idempotent API</li> <li>REST API protected by WAF with four AWS managed rules in production deployment</li> <li>Unit, infrastructure, security, integration and E2E tests.</li> <li>Automatically generated OpenAPI endpoint: /swagger with Pydantic schemas for both requests and responses</li> <li>Automated protection against API breaking changes</li> </ul> <p>The GitHub blueprint project can be found at https://github.com/ran-isenberg/aws-lambda-handler-cookbook.</p>"},{"location":"#serverless-best-practices","title":"Serverless Best Practices","text":"<p>The AWS Lambda handler will implement multiple best practice utilities.</p> <p>Each utility is implemented when a new blog post is published about that utility.</p> <p>The utilities cover multiple aspects of a production-ready service, including:</p> <ul> <li>Logging</li> <li>Observability: Monitoring and Tracing</li> <li>Observability: Business KPI Metrics</li> <li>Environment Variables</li> <li>Input Validation</li> <li>Dynamic configuration &amp; features flags</li> <li>Serverless Monitoring</li> <li>API Idempotency</li> <li>Learn How to Write AWS Lambda Functions with Three Architecture Layers</li> <li>Serverless OpenAPI Documentation with AWS Powertools</li> </ul> <p>While the code examples are written in Python, the principles are valid to any supported AWS Lambda handler programming language.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT License. See the LICENSE file.</p>"},{"location":"cdk/","title":"AWS CDK","text":""},{"location":"cdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Follow this getting started with CDK guide</li> <li>Make sure your AWS account and machine can deploy an AWS Cloudformation stack and have all the tokens and configuration as described in the page above.</li> <li>CDK Best practices blog</li> <li>Lambda layers best practices blog</li> </ul>"},{"location":"cdk/#cdk-deployment","title":"CDK Deployment","text":"<p>All CDK project files can be found under the CDK folder.</p> <p>The CDK code create an API GW with a path of /api/orders which triggers the lambda on 'POST' requests.</p> <p>The AWS Lambda handler uses a Lambda layer optimization which takes all the packages under the [packages] section in the Pipfile and downloads them in via a Docker instance.</p> <p>This allows you to package any custom dependencies you might have.</p> <p>In order to add a new dev dependency, add it to the Pipfile under the [tool.poetry.dependencies] section and run <code>poetry update -vvv</code>.</p> <p>In order to add a new Lambda runtime dependency, add it to the Pipfile under the [tool.poetry.dependencies] section and run <code>poetry update -vvv</code>.</p>"},{"location":"cdk/#cdk-constants","title":"CDK Constants","text":"<p>All AWS Lambda function configurations are saved as constants at the <code>cdk.service.constants.py</code> file and can easily be changed.</p> <ul> <li>Memory size</li> <li>Timeout in seconds</li> <li>Lambda dependencies build folder location</li> <li>Lambda Layer dependencies build folder location</li> <li>Various resources names</li> <li>Lambda function environment variables names and values</li> </ul>"},{"location":"cdk/#deployed-resources","title":"Deployed Resources","text":"<ul> <li>AWS Cloudformation stack: cdk.service.service_stack.py which is consisted of one construct</li> <li>Construct: cdk.service.api_construct.py which includes:<ul> <li>Lambda Layer - deployment optimization meant to be used with multiple handlers under the same API GW, sharing code logic and dependencies. You can read more about it here.</li> <li>Lambda Function - The Lambda handler function itself. Handler code is taken from the service <code>folder</code>.</li> <li>Lambda Role - The role of the Lambda function.</li> <li>API GW with Lambda Integration - API GW with a Lambda integration POST /api/orders that triggers the Lambda function.</li> <li>AWS DynamoDB table - stores request data. Created in the <code>api_db_construct.py</code> construct.</li> <li>AWS DynamoDB table - stores idempotency data. Created in the <code>api_db_construct.py</code> construct.</li> </ul> </li> <li>Construct: cdk.service.configuration.configuration_construct.py which includes:<ul> <li>AWS AppConfig configuration with an environment, application, configuration and deployment strategy. You can read more about it here.</li> </ul> </li> </ul>"},{"location":"cdk/#infrastructure-cdk-security-tests","title":"Infrastructure CDK &amp; Security Tests","text":"<p>Under tests there is an <code>infrastructure</code> folder for CDK infrastructure tests.</p> <p>The first test, <code>test_cdk</code> uses CDK's testing framework which asserts that required resources exists so the application will not break anything upon deployment.</p> <p>The security tests are based on <code>cdk_nag</code>. It checks your cloudformation output for security best practices. It can be found in the <code>service_stack.py</code> as part of the stack definition. It will fail the deployment when there is a security issue.</p> <p>For more information click here.</p>"},{"location":"cdk/#deployed-resources_1","title":"Deployed Resources","text":"<p>In the picture below you can see all the deployed resources ordered into domain groups. The image was created with the IDE plugin of AWS Application Composer.</p> <p></p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker - install Docker. Required for the Lambda layer packaging process.</li> <li>AWS CDK - Required for synth &amp; deploying the AWS Cloudformation stack. Run CDK Bootstrap on your AWS account and region.</li> <li>Python 3.13</li> <li>poetry - Make sure to have poetry v2 and above and to run <code>poetry config --local virtualenvs.in-project true</code> so all dependencies are installed in the project '.venv' folder.</li> <li>For Windows based machines, use the Makefile_windows version (rename to Makefile). Default Makefile is for Mac/Linux.</li> </ul>"},{"location":"getting_started/#getting-started","title":"Getting Started","text":"<p>You can start with a clean service out of this blueprint repository without using the 'Template' button on GitHub.</p> <p>You can use Cookiecutter.</p> <ul> <li>Cookiecutter - install with pip/brew <code>brew install cookiecutter</code> or <code>`pip install cookiecutter</code></li> </ul> <p>Then run:</p> <p><code>cookiecutter gh:ran-isenberg/cookiecutter-serverless-python</code></p> <p>Answer the questions to select repo name, service name, etc.:</p> <p></p> <p>That's it, your developer environment has been set! you are ready to deploy the service:</p>"},{"location":"getting_started/#creating-a-developer-environment-without-cookiecutter","title":"Creating a Developer Environment (without cookiecutter)","text":"<ol> <li>Run <code>make dev</code></li> </ol>"},{"location":"getting_started/#deploy-cdk","title":"Deploy CDK","text":"<p>Create a cloudformation stack by running <code>make deploy</code>.</p>"},{"location":"getting_started/#unit-tests","title":"Unit Tests","text":"<p>Unit tests can be found under the <code>tests/unit</code> folder.</p> <p>You can run the tests by using the following command: <code>make unit</code>.</p>"},{"location":"getting_started/#integration-tests","title":"Integration Tests","text":"<p>Make sure you deploy the stack first as these tests trigger your lambda handler LOCALLY but they can communicate with AWS services.</p> <p>These tests allow you to debug in your IDE your AWS Lambda function.</p> <p>Integration tests can be found under the <code>tests/integration</code> folder.</p> <p>You can run the tests by using the following command: <code>make integration</code>.</p>"},{"location":"getting_started/#e2e-tests","title":"E2E Tests","text":"<p>Make sure you deploy the stack first.</p> <p>E2E tests can be found under the <code>tests/e2e</code> folder.</p> <p>These tests send a 'POST' message to the deployed API GW and trigger the Lambda function on AWS.</p> <p>The tests are run automatically by: <code>make e2e</code>.</p>"},{"location":"getting_started/#deleting-the-stack","title":"Deleting the stack","text":"<p>CDK destroy can be run with <code>make destroy</code>.</p>"},{"location":"getting_started/#preparing-code-for-pr","title":"Preparing Code for PR","text":"<p>Run <code>make pr</code>. This command will run all the required checks, pre commit hooks, linters, code formatters, import sorting and tests, so you can be sure GitHub's pipeline will pass. It will also generate an updated swagger OpenAPI JSON file and place it at docs/swagger/openapi.json location.</p> <p>The command auto fixes errors in the code for you.</p> <p>If there's an error in the pre-commit stage, it gets auto fixed. However, are required to run <code>make pr</code> again so it continues to the next stages.</p> <p>Be sure to commit all the changes that <code>make pr</code> does for you.</p>"},{"location":"getting_started/#openapi-swagger-generation","title":"OpenAPI Swagger Generation","text":"<p>Run either <code>make pr</code> or <code>make openopi</code> to generate an updated swagger OpenAPI JSON file and place it at docs/swagger/openapi.json location.</p>"},{"location":"getting_started/#github-pages-documentation","title":"GitHub Pages Documentation","text":"<p><code>make docs</code> can be run to start a local HTTP server with the project's documentation pages.</p>"},{"location":"getting_started/#building-devlambda_requirementstxt","title":"Building dev/lambda_requirements.txt","text":""},{"location":"getting_started/#lambda_requirementstxt","title":"lambda_requirements.txt","text":"<p>CDK requires a requirements.txt in order to create a zip file with the Lambda layer dependencies. It's based on the project's poetry.lock file.</p> <p><code>make deploy</code> command will generate it automatically for you.</p>"},{"location":"getting_started/#dev_requirementstxt","title":"dev_requirements.txt","text":"<p>This file is used during GitHub CI to install all the required Python libraries without using poetry.</p> <p>File contents are created out of the Pipfile.lock.</p> <p><code>make deploy</code> and <code>make deps</code> are commands generate it automatically.</p>"},{"location":"pipeline/","title":"CI/CD Pipeline","text":""},{"location":"pipeline/#getting-started","title":"Getting Started","text":"<p>The GitHub CI/CD pipeline includes the following steps.</p> <p>The pipelines uses environment secrets (under the defined environment 'dev', 'staging' and 'production') for code coverage and for the role to deploy to AWS.</p> <p>When you clone this repository be sure to define the environments in your repo settings and create a secret per environment:</p> <ul> <li>AWS_ROLE - to role to assume for your GitHub worker as defined here</li> </ul>"},{"location":"pipeline/#makefile-commands","title":"Makefile Commands","text":"<p>All steps can be run locally using the makefile. See details below:</p> <ul> <li>Create Python environment</li> <li>Install dev dependencies</li> <li>Run pre-commit checks as defined in <code>.pre-commit-config.yaml</code></li> <li>Lint and format and sort imports with ruff (similar to flake8/yapf/isort) - run <code>make format</code> in the IDE</li> <li>Static type check with mypy as defined in <code>.mypy.ini</code> - run <code>make lint</code> in the IDE</li> <li>Verify that Python imports are sorted according to standard - run <code>make sort</code> in the IDE</li> <li>Python complexity checks: radon and xenon  - run <code>make complex</code> in the IDE</li> <li>Unit tests. Run <code>make unit</code> to run unit tests in the IDE</li> <li>Infrastructure test. Run <code>make infra-tests</code> to run the CDK infrastructure tests in the IDE</li> <li>Code coverage by codecov.io</li> <li>Deploy CDK - run <code>make deploy</code> in the IDE, will also run security tests based on cdk_nag</li> <li>Integration tests - run <code>make integration</code> in the IDE.</li> <li>E2E tests  - run <code>make e2e</code> in the IDE</li> <li>Code coverage tests  - run <code>make coverage-tests</code> in the IDE after CDK dep</li> <li>OpenAPI Swagger file - run</li> <li>Update GitHub documentation branch</li> <li>Update dependencies and CDK version automatically to the latest versions: run <code>make update-deps</code>.</li> </ul>"},{"location":"pipeline/#other-capabilities","title":"Other Capabilities","text":"<ul> <li>Automatic Python dependencies update with Dependabot</li> <li>Easy to use makefile allows to run locally all commands in the GitHub actions</li> <li>Run local docs server, prior to push in pipeline - run <code>make docs</code>  in the IDE</li> <li>Prepare PR, run all checks with one command - run <code>make pr</code> in the IDE</li> </ul>"},{"location":"pipeline/#environments-pipelines","title":"Environments &amp; Pipelines","text":"<p>All GitHub workflows are stored under <code>.github/workflows</code> folder.</p> <p>The two most important ones are <code>pr-serverless-service</code>  and <code>main-serverless-service</code>.</p>"},{"location":"pipeline/#pr-serverless-service","title":"pr-serverless-service","text":"<p><code>pr-serverless-service</code> runs for every pull request you open. It expects you defined a GitHub environments by the name <code>dev</code>, <code>staging</code> and <code>production</code> and for each environments to have the following variables:  <code>CODECOV_TOKEN</code> for CodeCov integration and <code>AWS_ROLE</code> that allows GitHub to deploy to that AWS account (one for dev, one for staging and one for production accounts).</p> <p>It includes two jobs: 'quality_standards' and 'tests' where a failure in 'quality_standards' does not trigger 'tests'. Both jobs MUST pass in order to to be able to merge.</p> <p>'quality_standards' includes all linters, pre-commit checks and units tests and 'tests' deploys the service to AWS, runs code coverage checks,</p> <p>checks that your OpenAPI file is up to date and corresponds to the actual deployment (fails if it does not, hence your documentation is out of date), security checks and E2E tests. Stack is destroyed at the end. Stack has a 'dev'</p> <p>prefix as part of its name. Each environment has a pre-defined stack prefix.</p> <p>Once merged, <code>main-serverless-service</code> will run.</p>"},{"location":"pipeline/#main-serverless-service","title":"main-serverless-service","text":"<p><code>main-serverless-service</code> runs for every MERGED pull request that runs on the main branch. It expects you defined a GitHub environments by the name <code>staging</code> and <code>production</code> and that both includes a secret by the name of <code>AWS_ROLE</code>.</p> <p>It includes three jobs: 'staging', 'production' and 'publish_github_pages'.</p> <p>'staging' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It runs just coverage tests and E2E tests. Stack is not deleted. Stack has a 'staging' prefix as part of its name. Any failure in staging will stop the pipeline and production environment will not get updated with the new code.</p> <p>'production' does not run any of the 'quality_standards' checks, since they were already checked before the code was merged. It does not run any test at the moment. Stack is not deleted. Stack has a 'production' prefix as part of its name.</p>"},{"location":"best_practices/dynamic_configuration/","title":"Dynamic Configuration & Smart Feature Flags","text":"<p>Feature flags are used to modify behavior without changing the application's code.</p> <p>This pages describes a utility for fetching dynamic configuration and evaluating smart feature flags stored in AWS AppConfig as a JSON file.</p> <p>This utility is based on the Feature Flags utility of AWS Lambda Powertools Github repository that I designed and developed.</p>"},{"location":"best_practices/dynamic_configuration/#blog-reference","title":"Blog Reference","text":"<p>Read more about the differences between static and dynamic configurations, when to use each type how this utility works. Click HERE</p>"},{"location":"best_practices/dynamic_configuration/#key-features","title":"Key features","text":"<ul> <li>CDK construct that deploys your JSON configuration to AWS AppConfig</li> <li>Uses a JSON file to describe both configuration values and smart feature flags.</li> <li>Provides one simple API to get configuration anywhere in the AWS Lambda function code.</li> <li>Provides one simple API to evaluate smart feature flags values.</li> <li>During runtime, stores the configuration in a local cache with a configurable TTL to reduce API calls to AWS (to fetch the JSON configuration) and total cost.</li> <li>Built-in support for Pydantic models. We've used Pydantic to serialize and validate JSON configuration (input validation and environment variables) throughout this blog series, so it makes sense to use it to parse dynamic configuration.</li> </ul>"},{"location":"best_practices/dynamic_configuration/#what-is-aws-appconfig","title":"What is AWS AppConfig","text":"<p>AWS AppConfig is a self-managed service that stores plain text/YAML/JSON configuration to be consumed by multiple clients.</p> <p>We will use it in the context of dynamic configuration and feature toggles and store a single JSON file that contains both feature flags and configuration values.</p> <p>Let's review its advantages:</p> <ul> <li>FedRAMP High certified</li> <li>Fully Serverless</li> <li>Out of the box support for schema validations that run before a configuration update.</li> <li>Out-of-the-box integration with AWS CloudWatch alarms triggers an automatic configuration revert if a configuration update fails your AWS Lambda functions. Read more about it here.</li> <li>You can define configuration deployment strategies. Deployment strategies define how and when to change a configuration. Read more about it here.</li> <li>It provides a single API that provides configuration and feature flags access\u2014more on that below.</li> <li>AWS AppConfig provides integrations with other services such as Atlassian Jira and AWS CodeDeploy.</li> </ul>"},{"location":"best_practices/dynamic_configuration/#cdk-construct","title":"CDK Construct","text":"<p>'configuration_construct.py' defines a CDK v2 AWS AppConfig configuration with the following entities:</p> <ol> <li>Application (service)</li> <li>Environment</li> <li>Custom deployment strategy - immediate deploy, 0 minutes wait, no validations or AWS CloudWatch alerts</li> <li>The JSON configuration. It uploads the files \u2018cdk/service/configuration/json/{environment}_configuration.json\u2019, where environment is a construct argument (default is 'dev')</li> </ol> <p>The construct validates the JSON file and verifies that feature flags syntax is valid and exists under the 'features' key. Feature flags are optional.</p> <p>Make sure to deploy this construct in a separate pipeline from the AWS Lambda function (unlike in this example), otherwise it wont be a dynamic configuration.</p> <p>Read more about AWS AppConfig here.</p>"},{"location":"best_practices/dynamic_configuration/#configuration-stack-example","title":"Configuration Stack Example","text":"<p>Args:</p> <ul> <li>scope (Construct): The scope in which to define this construct.</li> <li>id_ (str): The scoped construct ID. Must be unique amongst siblings. If the ID includes a path separator (<code>/</code>), then it will be replaced by double dash <code>--</code>.</li> <li>environment (str): environment name. Used for loading the corresponding JSON file to upload under 'configuration/json/{environment}_configuration.json'</li> <li>service_name (str): application name.</li> <li>configuration_name (str): configuration name</li> <li>deployment_strategy_id (str, optional): AWS AppConfig deployment strategy.</li> </ul> configuration_stack.py <pre><code>from aws_cdk import Stack\nfrom constructs import Construct\n\nfrom cdk.service.configuration.configuration_construct import ConfigurationStore\nfrom cdk.service.constants import CONFIGURATION_NAME, ENVIRONMENT, SERVICE_NAME\n\n\nclass DynamicConfigurationStack(Stack):\n    def __init__(self, scope: Construct, id: str, **kwargs) -&gt; None:\n        super().__init__(scope, id, **kwargs)\n\n        self.dynamic_configuration = ConfigurationStore(self, 'dynamic_conf', ENVIRONMENT, SERVICE_NAME, CONFIGURATION_NAME)\n</code></pre> <p>The JSON configuration that is uploaded to AWS AppConfig resides under <code>cdk/service/configuration/json/dev_configuration.json</code></p> <p><code>dev</code> represents the default environment. You can add multiple configurations for different environments.</p> <p>Make sure the prefix remains the environment underscore configuration <code>dev_configuration</code>, <code>prod_configuration</code> etc.</p>"},{"location":"best_practices/dynamic_configuration/#aws-lambda-cdk-changes","title":"AWS Lambda CDK Changes","text":"<p>You need to add two new settings in order to use this utility:</p> <ol> <li>New environment variables:</li> <li>AWS AppConfig configuration application name (\u2018CONFIGURATION_APP\u2019)</li> <li>AWS AppConfig environment name (\u2018CONFIGURATION_ENV\u2019)</li> <li>AWS AppConfig configuration name to fetch (\u2018CONFIGURATION_NAME\u2019)</li> <li>Cache TTL in minutes (\u2018CONFIGURATION_MAX_AGE_MINUTES\u2019)</li> <li>AWS Lambda IAM role to include allow 'appconfig:GetLatestConfiguration' and 'appconfig:StartConfigurationSession' on '*'.</li> </ol> cdk_lambda.py <pre><code>from aws_cdk import Duration\nfrom aws_cdk import aws_dynamodb as dynamodb\nfrom aws_cdk import aws_iam as iam\nfrom aws_cdk import aws_lambda as _lambda\n\nimport cdk.service.constants as constants\n\n\ndef _build_lambda_role(self, db: dynamodb.Table) -&gt; iam.Role:\n    return iam.Role(\n        self,\n        constants.SERVICE_ROLE,\n        assumed_by=iam.ServicePrincipal('lambda.amazonaws.com'),\n        inline_policies={\n            'dynamic_configuration': iam.PolicyDocument(\n                statements=[\n                    iam.PolicyStatement(\n                        actions=['appconfig:GetLatestConfiguration', 'appconfig:StartConfigurationSession'],\n                        resources=['*'],\n                        effect=iam.Effect.ALLOW,\n                    )\n                ]\n            ),\n            'dynamodb_db': iam.PolicyDocument(\n                statements=[iam.PolicyStatement(actions=['dynamodb:PutItem', 'dynamodb:GetItem'], resources=[db.table_arn], effect=iam.Effect.ALLOW)]\n            ),\n        },\n        managed_policies=[\n            iam.ManagedPolicy.from_aws_managed_policy_name(managed_policy_name=(f'service-role/{constants.LAMBDA_BASIC_EXECUTION_ROLE}'))\n        ],\n    )\n\n\ndef _build_lambda_function(self, role: iam.Role, db: dynamodb.Table, appconfig_app_name: str) -&gt; _lambda.Function:\n    return _lambda.Function(\n        self,\n        'ServicePost',\n        runtime=_lambda.Runtime.PYTHON_3_13,\n        code=_lambda.Code.from_asset(constants.BUILD_FOLDER),\n        handler='service.handlers.create_order.create_order',\n        environment={\n            constants.POWERTOOLS_SERVICE_NAME: constants.SERVICE_NAME,  # for logger, tracer and metrics\n            constants.POWER_TOOLS_LOG_LEVEL: 'DEBUG',  # for logger\n            'REST_API': 'https://www.ranthebuilder.cloud/api',  # for env vars example\n            'ROLE_ARN': 'arn:partition:service:region:account-id:resource-type:resource-id',  # for env vars example\n            'CONFIGURATION_APP': appconfig_app_name,  # for feature flags\n            'CONFIGURATION_ENV': constants.ENVIRONMENT,\n            'CONFIGURATION_NAME': constants.CONFIGURATION_NAME,\n            'CONFIGURATION_MAX_AGE_MINUTES': constants.CONFIGURATION_MAX_AGE_MINUTES,\n            'TABLE_NAME': db.table_name,\n        },\n        tracing=_lambda.Tracing.ACTIVE,\n        retry_attempts=0,\n        timeout=Duration.seconds(constants.API_HANDLER_LAMBDA_TIMEOUT),\n        memory_size=constants.API_HANDLER_LAMBDA_MEMORY_SIZE,\n        role=role,\n    )\n</code></pre>"},{"location":"best_practices/dynamic_configuration/#fetching-dynamic-configuration","title":"Fetching Dynamic Configuration","text":"<p>You need to model your dynamic configuration as a Pydantic schema class (excluding feature flags) extend Pydantic's BaseModel class.</p> <p>The <code>parse_configuration</code> function will fetch the JSON configuration from AWS AppConfig and use your Pydantic model to validate it and return a valid dataclass instance.</p> <p>Let's assume that our configuration looks like this:</p> configuration.json <pre><code>{\n    \"countries\": [\n        \"ISRAEL\",\n        \"USA\"\n    ]\n}\n</code></pre> <p>We need to define a Pydantic model that will parse and validate the JSON file and pass it as an argument to the function <code>parse_configuration</code>.</p> configuration_schema.py <pre><code>from pydantic import BaseModel\n\n\n# does not include feature flags part of the JSON\nclass MyConfiguration(BaseModel):\n    countries: list[str]\n</code></pre> <p>Now we can call the <code>parse_configuration</code> function and pass it the <code>MyConfiguration</code> class name.</p> <p>The function fetch the JSON file from AWS AppConfig and return a parsed instance of the configuration dataclass <code>MyConfiguration</code>.</p> my_handler.py <pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.dynamic_configuration import MyConfiguration\nfrom service.handlers.models.env_vars import MyHandlerEnvVars\nfrom service.handlers.utils.dynamic_configuration import parse_configuration\nfrom service.handlers.utils.observability import logger\n\n\ndef build_response(http_status: HTTPStatus, body: dict[str, Any]) -&gt; dict[str, Any]:\n    return {'statusCode': http_status, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps(body)}\n\n\n@init_environment_variables(model=MyHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    try:\n        my_configuration = parse_configuration(model=MyConfiguration)\n    except Exception:\n        logger.exception('dynamic configuration error')\n        return build_response(http_status=HTTPStatus.INTERNAL_SERVER_ERROR, body={})\n\n    logger.debug('fetched dynamic configuration', countries=my_configuration.countries)\n    return build_response(http_status=HTTPStatus.OK, body={'message': 'success'})\n</code></pre> <p>If you want to learn more about how <code>parse_configuration</code> function works, click here.</p>"},{"location":"best_practices/dynamic_configuration/#feature-flags","title":"Feature Flags","text":"<p>Feature flags can be evaluated to any valid JSON value.</p> <p>However, in these snippets, they are all boolean for simplicity.</p>"},{"location":"best_practices/dynamic_configuration/#smart-feature-flags","title":"Smart Feature Flags","text":"<p>Smart feature flags are feature flags that are evaluated in runtime and can change their values according to session context.</p> <p>Smart feature flags require evaluation in runtime and can have different values for different AWS Lambda function sessions.</p> <p>Imagine pushing a new feature into production but enabling it only for several specific customers.</p> <p>A smart feature flag will need to evaluate the customer name and decide whether the final value is 'True/False'.</p> <p>Smart feature flags are defined by rules, conditions, and actions determining the final value.</p> <p>Read more about them here</p>"},{"location":"best_practices/dynamic_configuration/#regular-feature-flags","title":"Regular Feature Flags","text":"<p>Regular feature flags are flags that have a default value which does not change according to input context.</p>"},{"location":"best_practices/dynamic_configuration/#evaluating-feature-flags","title":"Evaluating Feature Flags","text":"<p>It is mandatory to store feature flags under the 'features' key in the configuration JSON as the utilities described here are configured as such.</p> <p>Feature flags can have any valid JSON value but these examples use boolean.</p> <p>Let's assume that our AWS Lambda handler supports two feature flags: regular and smart.</p> <ol> <li>Ten percent discount for the current order: <code>True/False</code>.</li> <li>Premium feature for the customer: <code>True/False</code>.</li> </ol> <p>Premium features are enabled only to specific customers. A ten percent discount is a simple feature flag. According to store policy, a ten percent discount can be turned on or off.</p> <p>It doesn't change according to session input; it is <code>True</code> or <code>False</code> for all inputs.</p> <p>On the other hand,  premium features are based on a rule. It's a smart feature flag.Its' value is <code>False</code> for all but specific customers.</p> <p>To use AWS Lambda Powertools feature flags capabilities, we need to build a JSON file that matches the SDK language.</p>"},{"location":"best_practices/dynamic_configuration/#regular-feature-flags-definition","title":"Regular Feature Flags Definition","text":"<p>Defining the ten percent discount flag is simple. It has a key and a dictionary containing a <code>default value</code> key with a boolean value.</p> <p>Let's assume the feature flags are enabled. Let's add it to the current configuration we already have:</p> configuration.json <pre><code>{\n    \"countries\": [\n        \"ISRAEL\",\n        \"USA\"\n    ]\n}\n</code></pre>"},{"location":"best_practices/dynamic_configuration/#smart-feature-flags-json-definition","title":"Smart Feature Flags JSON Definition","text":"<p>Now, let's add the smart feature flag, premium features. We want to enable it only for customers by 'RanTheBuilder.'</p> <p>The JSON structure is simple.</p> <p>Each feature has a default value under the default key. It can any valid JSON value (boolean, int etc.).</p> <p>Each feature can have optional rules that determine the evaluated value.</p> <p>Each rule consists of a default value to return (in case of a match \u2014 when_match ) and a list of conditions. Only one rule can match.</p> <p>Each condition consists of an action name (which is mapped to an operator in the rule engine code) and a key-value pair that serves as an argument to the SDK rule engine.</p> <p>The combined JSON configuration look like this:</p> configuration.json <pre><code>{\n    \"features\": {\n        \"premium_features\": {\n            \"default\": false,\n            \"rules\": {\n                \"enable premium features for this specific customer name\": {\n                    \"when_match\": true,\n                    \"conditions\": [\n                        {\n                            \"action\": \"EQUALS\",\n                            \"key\": \"customer_name\",\n                            \"value\": \"RanTheBuilder\"\n                        }\n                    ]\n                }\n            }\n        },\n        \"ten_percent_off_campaign\": {\n            \"default\": true\n        }\n    },\n    \"countries\": [\n        \"ISRAEL\",\n        \"USA\"\n    ]\n}\n</code></pre>"},{"location":"best_practices/dynamic_configuration/#api-example","title":"API Example","text":"my_handler.py <pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.dynamic_configuration import MyConfiguration\nfrom service.handlers.models.env_vars import MyHandlerEnvVars\nfrom service.handlers.utils.dynamic_configuration import get_configuration_store, parse_configuration\nfrom service.handlers.utils.observability import logger\n\n\n@init_environment_variables(model=MyHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    try:\n        my_configuration: MyConfiguration = parse_configuration(model=MyConfiguration)  # type: ignore\n        logger.debug('fetched dynamic configuration', configuration=my_configuration.model_dump())\n    except Exception:\n        logger.exception('dynamic configuration error')\n        return {'statusCode': HTTPStatus.INTERNAL_SERVER_ERROR, 'headers': {'Content-Type': 'application/json'}, 'body': ''}\n\n    campaign = get_configuration_store().evaluate(\n        name='ten_percent_off_campaign',\n        context={},\n        default=False,\n    )\n    logger.debug('campaign feature flag value', campaign=campaign)\n\n    premium = get_configuration_store().evaluate(\n        name='premium_features',\n        context={'customer_name': 'RanTheBuilder'},\n        default=False,\n    )\n    logger.debug('premium feature flag value', premium=premium)\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre> <p>In this example, we evaluate both feature flags' value and provide a context.</p> <p><code>ten_percent_off_campaign</code> will be evaluated to <code>True</code> since it's a non smart feature flag with a default value of <code>False</code> in the JSON configuration.</p> <p>The rule for <code>premium_features</code> is matched (which returns a <code>True</code> value for the flag) when the context dictionary has a key <code>customer_name</code> with a value of <code>RanTheBuilder</code> EQUALS <code>RanTheBuilder</code>.</p> <p>Line 30 will return a value of <code>True</code> for the context <code>{'customer_name': 'RanTheBuilder'}</code> and <code>False</code> for any other input.</p> <p>There are several actions for conditions such as <code>STARTSWITH</code>, <code>ENDSWITH</code>, <code>EQUALS</code>, etc.</p> <p>You can read more about the rules, conditions, logic, and supported actions here.</p>"},{"location":"best_practices/dynamic_configuration/#how-to-locally-test-your-lambda-in-ide","title":"How To Locally Test Your Lambda In IDE","text":"<p>You can and should mock the values that AWS AppConfig returns in order to check different types of configurations values and feature flags.</p> <p>Make sure to always test your feature flags with all its possible values scope (enabled/disabled etc.)</p> <p>You can also skip the mock and read the real values that are currently stored in AWS AppConfig.</p> <p>However, I'd do that in the E2E tests.</p> mock_test.py <pre><code>from typing import Any\n\nMOCKED_SCHEMA = {\n    'features': {\n        'premium_features': {\n            'default': False,\n            'rules': {\n                'enable premium features for this specific customer name\"': {\n                    'when_match': True,\n                    'conditions': [{'action': 'EQUALS', 'key': 'customer_name', 'value': 'RanTheBuilder'}],\n                }\n            },\n        },\n        'ten_percent_off_campaign': {'default': True},\n    },\n    'countries': ['ISRAEL', 'USA'],\n}\n\n\ndef mock_dynamic_configuration(mocker, mock_schema: dict[str, Any]) -&gt; None:\n    \"\"\"Mock AppConfig Store get_configuration method to use mock schema instead\"\"\"\n    mocked_get_conf = mocker.patch('aws_lambda_powertools.utilities.parameters.AppConfigProvider.get')\n    mocked_get_conf.return_value = mock_schema\n\n\ndef my_test(mocker):\n    mock_dynamic_configuration(mocker, MOCKED_SCHEMA)\n    # start test\n</code></pre> <p>Click here for more details.</p>"},{"location":"best_practices/dynamic_configuration/#extra-documentation","title":"Extra Documentation","text":"<p>Read here about Pydantic field types.</p> <p>Read here about custom validators and advanced value constraints.</p>"},{"location":"best_practices/environment_variables/","title":"Environment Variables","text":"<p>Environment Variables decorator is a simple parser for environment variables that run at the start of the handler invocation.</p> <p></p>"},{"location":"best_practices/environment_variables/#key-features","title":"Key features","text":"<ul> <li>A defined Pydantic schema for all required environment variables</li> <li>A decorator that parses and validates environment variables, value constraints included</li> <li>Global getter for parsed &amp; valid schema dataclass with all environment variables</li> </ul> <p>The best practice for handling environment variables is to validate &amp; parse them according to a predefined schema as soon as the AWS Lambda function is triggered.</p> <p>In case of misconfiguration, a validation exception is raised with all the relevant exception details.</p>"},{"location":"best_practices/environment_variables/#open-source","title":"Open source","text":"<p>The code in this post has been moved to an open source project you can use:</p> <p>The AWS Lambda environment variables modeler</p>"},{"location":"best_practices/environment_variables/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of validating environment variables and how this utility works. Click HERE</p>"},{"location":"best_practices/environment_variables/#schema-definition","title":"Schema Definition","text":"<p>You need to define all your environment variables in a Pydantic schema class that extend Pydantic's BaseModel class.</p> <p>For example:</p> schemas/env_vars.py<pre><code>from typing import Annotated, Literal\n\nfrom pydantic import BaseModel, Field, HttpUrl\n\n\nclass MyHandlerEnvVars(BaseModel):\n    REST_API: HttpUrl\n    ROLE_ARN: Annotated[str, Field(min_length=20, max_length=2048)]\n    POWERTOOLS_SERVICE_NAME: Annotated[str, Field(min_length=1)]\n    LOG_LEVEL: Literal['DEBUG', 'INFO', 'ERROR', 'CRITICAL', 'WARNING', 'EXCEPTION']\n</code></pre> <p>All Pydantic schemas extend Pydantic\u2019s \u2018BaseModel\u2019 class, turning them into a dataclass.</p> <p>The schema defines four environment variables: \u2018LOG_LEVEL,\u2019 \u2018POWERTOOLS_SERVICE_NAME,\u2019 \u2018ROLE_ARN,\u2019 and \u2018REST_API.\u2019</p> <p>This schema makes sure that:</p> <ul> <li>\u2018LOG_LEVEL\u2019 is one of the strings in the Literal list.</li> <li>\u2018ROLE_ARN\u2019 exists and is between 20 and 2048 characters long, as defined here.</li> <li>\u2018REST_API\u2019 is a valid HTTP URL.</li> <li>\u2018POWERTOOLS_SERVICE_NAME\u2019 is a non-empty string.</li> </ul> <p>Read here about Pydantic Model capabilities.</p>"},{"location":"best_practices/environment_variables/#decorator-usage","title":"Decorator Usage","text":"<p>The decorator 'init_environment_variables' is defined under the utility folder service.utils.env_vars_parser.py and imported in the handler.</p> <p>The decorator requires a model parameter, which in this example is the name of the schema class we defined above.</p> handlers/my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import get_environment_variables, init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.env_vars import MyHandlerEnvVars\n\n\n@init_environment_variables(model=MyHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    env_vars = get_environment_variables(model=MyHandlerEnvVars)  # noqa: F841\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/environment_variables/#global-getter-usage","title":"Global Getter Usage","text":"<p>The getter function 'get_environment_variables' is defined under the utility folder service.utils.env_vars_parser.py and imported in the handler.</p> <p>The getter function returns a parsed and validated global instance of the environment variables Pydantic schema class.</p> <p>It can be used anywhere in the function code, not just the handler.</p> handlers/my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_env_modeler import get_environment_variables, init_environment_variables\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom service.handlers.models.env_vars import MyHandlerEnvVars\n\n\n@init_environment_variables(model=MyHandlerEnvVars)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    env_vars: MyHandlerEnvVars = get_environment_variables(model=MyHandlerEnvVars)  # noqa: F841\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/environment_variables/#more-details","title":"More Details","text":"<p>Read here about Pydantic field types.</p> <p>Read here about custom validators and advanced value constraints.</p>"},{"location":"best_practices/input_validation/","title":"Input Validation","text":"<p>This utility provides input validation and advanced parsing. It mitigates any hidden input assumption including value constraints.</p> <p>Rule of thumb: Always, always, always (!), validate all input.</p> <p></p>"},{"location":"best_practices/input_validation/#key-features","title":"Key features","text":"<p>The Parser will validate the incoming event, extract the input business payload, decode it and validate it according to a predefined schema.</p> <p>This schema will verify that all required parameters exist, their type is as expected and validate any value constraints.</p> <p>And all this will be achieved with one line of code.</p> <p>This \"magic\" line will allow you to focus on your business logic and not worry about metadata and encapsulating services.</p>"},{"location":"best_practices/input_validation/#service-envelope","title":"Service Envelope","text":"<p>When an AWS Service sends an event that triggers your AWS Lambda function, metadata information is added to the event, and the business logic payload is encapsulated.</p> <p>Let's call this metadata information 'envelope.'</p> <p>The envelope contains valuable information, interesting headers, and the interesting part, the business logic input that you wish to process.</p> <p>That's where it gets tricky.</p> <p>Each AWS service has its envelope structure and may encapsulate the business logic payload differently.</p>"},{"location":"best_practices/input_validation/#supported-aws-services","title":"Supported AWS Services","text":"<p>For a complete list click here.</p>"},{"location":"best_practices/input_validation/#define-business-logic-schema","title":"Define Business Logic Schema","text":"<p>Use Pydantic schemas to define the expected input format. Extend 'BaseModel' class.</p> <p>Define type and value constraints.</p> schemas/input.py <pre><code>from pydantic import BaseModel, PositiveInt, constr\n\nclass Input(BaseModel):\n    customer_name: constr(min_length=1, max_length=20)\n    order_item_count: PositiveInt\n</code></pre> <p>The schema defines:</p> <ol> <li>'customer_name' - customer name, a non-empty string with up to 20 characters.</li> <li>'order_item_count' - a positive integer representing the number of ordered items that 'customer_name' placed.</li> </ol> <p>Learn about models here and about advanced parsing here.</p>"},{"location":"best_practices/input_validation/#usage-in-handler","title":"Usage in Handler","text":"<p>The parser is a called with the function 'parse'.</p> <p>Use the envelope class that matches the AWS service that triggers your AWS Lambda function.</p> my_handler.py<pre><code>from http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.utilities.parser import ValidationError, parse\nfrom aws_lambda_powertools.utilities.parser.envelopes import ApiGatewayEnvelope\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nfrom .schema import Input\n\n\ndef my_handler(event: dict[str, Any], context: LambdaContext):\n    try:\n        input: Input = parse(event=event, model=Input, envelope=ApiGatewayEnvelope)  # noqa: F841\n    except (ValidationError, TypeError):\n        # log error, return BAD_REQUEST\n        return {'statusCode': HTTPStatus.BAD_REQUEST}\n    # process input\n</code></pre>"},{"location":"best_practices/input_validation/#accessing-envelope-metadata","title":"Accessing Envelope Metadata","text":"<p>You can access the metadata parameters by extending the model class and parsing the input without the envelope class.</p> <p>Read more about it here.</p>"},{"location":"best_practices/input_validation/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of input validation and the potential pitfalls it prevents in my blog. Click HERE.</p>"},{"location":"best_practices/input_validation/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/utilities/parser/</p>"},{"location":"best_practices/logger/","title":"Logger","text":"<p>It\u2019s a wrapper of Python\u2019s logging library that provides extra capabilities such as JSON output configuration (and many more).</p> <p></p>"},{"location":"best_practices/logger/#key-features","title":"Key features","text":"<ul> <li>Capture key fields from Lambda context, cold start and structures logging output as JSON</li> <li>Log Lambda event when instructed (disabled by default)</li> <li>Append additional keys to structured log at any point in time</li> </ul>"},{"location":"best_practices/logger/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.logging.logger import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger: Logger = Logger(service='service')  # JSON output format, service name can be set by environment variable \"POWERTOOLS_SERVICE_NAME\"\n\n\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    logger.set_correlation_id(context.aws_request_id)\n    logger.debug('my_handler is called')\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/logger/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of the logger and how to use AWS CloudWatch logs in my blog. Click HERE</p>"},{"location":"best_practices/logger/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/logger/</p>"},{"location":"best_practices/metrics/","title":"Metrics","text":"<p>Business metrics and KPIs can drive your business forward towards success.</p> <p>Metrics consist of a key and a value; A value can be a number, percentage, rate, or any other unit. Typical metrics are the number of sessions, users, error rate, number of views, etc.</p> <p>The Metrics utility creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF).</p> <p>These metrics can be visualized through Amazon CloudWatch Console.</p> <p></p>"},{"location":"best_practices/metrics/#key-features","title":"Key features","text":"<ul> <li>Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob)</li> <li>Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc)</li> <li>Metrics are created asynchronously by CloudWatch service, no custom stacks needed</li> </ul>"},{"location":"best_practices/metrics/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.metrics import Metrics, MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSERVICE_NAME = 'service'\n\n# namespace and service name can be set by environment variable \"POWERTOOLS_METRICS_NAMESPACE\" and \"POWERTOOLS_SERVICE_NAME\" accordingly\nmetrics = Metrics(namespace='my_product_kpi', service=SERVICE_NAME)\n\n\n@metrics.log_metrics\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    metrics.add_metric(name='ValidEvents', unit=MetricUnit.Count, value=1)\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/metrics/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of the business KPis and metrics in my blog. Click HERE</p>"},{"location":"best_practices/metrics/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/metrics/</p>"},{"location":"best_practices/monitoring/","title":"Monitoring","text":""},{"location":"best_practices/monitoring/#key-concepts","title":"Key Concepts","text":"<p>Utilizing AWS CloudWatch dashboards enables centralized monitoring of API Gateway, Lambda functions, and DynamoDB, providing real-time insights into their performance and operational health. By aggregating metrics, logs, and alarms, CloudWatch facilitates swift issue diagnosis and analysis across your serverless applications. Additionally, setting up alarms ensures immediate alerts during anomalous activities, enabling proactive issue mitigation.</p>"},{"location":"best_practices/monitoring/#service-architecture","title":"Service Architecture","text":"<p>The goal is to monitor the service API gateway, Lambda function, and DynamoDB tables and ensure everything is in order.</p> <p>In addition, we want to visualize service KPI metrics.</p>"},{"location":"best_practices/monitoring/#monitoring-dashboards","title":"Monitoring Dashboards","text":"<p>We will define two dashboards:</p> <ul> <li>High level</li> <li>Low level</li> </ul> <p>Each dashboard has its usage and tailors different personas' usage.</p>"},{"location":"best_practices/monitoring/#high-level-dashboard","title":"High Level Dashboard","text":"<p>This dashboard is designed to be an executive overview of the service.</p> <p>Total API gateway metrics provide information on the performance and error rate of the service.</p> <p>KPI metrics are included in the bottom part as well.</p> <p>Personas that use this dashboard: SRE, developers, and product teams (KPIs)</p>"},{"location":"best_practices/monitoring/#low-level-dashboard","title":"Low Level Dashboard","text":"<p>It is aimed at a deep dive into all the service's resources. Requires an understanding of the service architecture and its moving parts.</p> <p>The dashboard provides the Lambda function's metrics for latency, errors, throttles, provisioned concurrency, and total invocations.</p> <p>In addition, a CloudWatch logs widget shows only 'error' logs from the Lambda function.</p> <p>As for DynamoDB tables, we have the primary database and the idempotency table for usage, operation latency, errors, and throttles.</p> <p>Personas that use this dashboard: developers, SREs.</p>"},{"location":"best_practices/monitoring/#alarms","title":"Alarms","text":"<p>Having visibility and information is one thing, but being proactive and knowing beforehand that a significant error is looming is another. A CloudWatch</p> <p>Alarm is an automated notification tool within AWS CloudWatch that triggers alerts based on user-defined thresholds, enabling users to identify and</p> <p>respond to operational issues, breaches, or anomalies in AWS resources by monitoring specified metrics over a designated period.</p> <p>In this dashboard, you will find an example of two types of alarms:</p> <ul> <li>Alarm for performance threshold monitoring</li> <li>Alarm for error rate threshold monitoring</li> </ul> <p>For latency-related issues, we define the following alarm:</p> <p></p> <p>For P90, P50 metrics, follow this explanation.</p> <p>For internal server errors rate, we define the following alarm: </p>"},{"location":"best_practices/monitoring/#actions","title":"Actions","text":"<p>Alarms are of no use unless they have an action. We have configured the alarms to send an SNS notification to a new SNS topic. From there, you can connect any subscription - HTTPS/SMS/Email, etc. to notify your teams with the alarm details.</p>"},{"location":"best_practices/monitoring/#cdk-reference","title":"CDK Reference","text":"<p>We use the open-source cdk-monitoring-constructs.</p> <p>You can view find the monitoring CDK construct here.</p>"},{"location":"best_practices/monitoring/#further-reading","title":"Further Reading","text":"<p>If you wish to learn more about this concept and go over details on the CDK code, check out my blog post.</p>"},{"location":"best_practices/tracer/","title":"Tracer","text":"<p>Tracer is a thin wrapper for AWS X-Ray Python SDK.</p> <p></p>"},{"location":"best_practices/tracer/#key-features","title":"Key features","text":"<ul> <li>Enables AWS X-Ray traces for your handler by using simple and clean decorators</li> <li>Captures handler and inner functions</li> <li>Auto capture cold start as annotation, and responses or full exceptions as metadata</li> </ul>"},{"location":"best_practices/tracer/#usage-in-handler","title":"Usage in Handler","text":"my_handler.py<pre><code>import json\nfrom http import HTTPStatus\nfrom typing import Any\n\nfrom aws_lambda_powertools.tracing.tracer import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSERVICE_NAME = 'service'\n\n# service name can be set by environment variable \"POWERTOOLS_SERVICE_NAME\". Disabled by setting POWERTOOLS_TRACE_DISABLED to \"True\"\ntracer: Tracer = Tracer(service=SERVICE_NAME)\n\n\n@tracer.capture_method(capture_response=False)\ndef inner_function_example(event: dict[str, Any]) -&gt; dict[str, Any]:\n    return {}\n\n\n@tracer.capture_lambda_handler(capture_response=False)\ndef my_handler(event: dict[str, Any], context: LambdaContext) -&gt; dict[str, Any]:\n    inner_function_example(event)\n    return {'statusCode': HTTPStatus.OK, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'message': 'success'})}\n</code></pre>"},{"location":"best_practices/tracer/#blog-reference","title":"Blog Reference","text":"<p>Read more about the importance of observability and traces in my blog. Click HERE</p>"},{"location":"best_practices/tracer/#more-details","title":"More Details","text":"<p>You can find more information at the official documentation.</p> <p>Go to https://docs.powertools.aws.dev/lambda-python/latest/core/tracer/</p>"},{"location":"swagger/swagger/","title":"Swagger","text":""}]}